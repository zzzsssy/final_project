{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0360899-4745-43bf-87e8-cee4c98d03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b8cd0f-72d6-4259-a584-cc4d6a183170",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Public\\Documents\\creditcard_2023.csv')\n",
    "data = data.drop(['id'], axis=1)\n",
    "data = data.drop(['Amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82adb6a0-b0f0-414f-bf85-1672dc567f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.260648</td>\n",
       "      <td>-0.469648</td>\n",
       "      <td>2.496266</td>\n",
       "      <td>-0.083724</td>\n",
       "      <td>0.129681</td>\n",
       "      <td>0.732898</td>\n",
       "      <td>0.519014</td>\n",
       "      <td>-0.130006</td>\n",
       "      <td>0.727159</td>\n",
       "      <td>0.637735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091202</td>\n",
       "      <td>-0.110552</td>\n",
       "      <td>0.217606</td>\n",
       "      <td>-0.134794</td>\n",
       "      <td>0.165959</td>\n",
       "      <td>0.126280</td>\n",
       "      <td>-0.434824</td>\n",
       "      <td>-0.081230</td>\n",
       "      <td>-0.151045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.985100</td>\n",
       "      <td>-0.356045</td>\n",
       "      <td>0.558056</td>\n",
       "      <td>-0.429654</td>\n",
       "      <td>0.277140</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.406466</td>\n",
       "      <td>-0.133118</td>\n",
       "      <td>0.347452</td>\n",
       "      <td>0.529808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233984</td>\n",
       "      <td>-0.194936</td>\n",
       "      <td>-0.605761</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>-0.577395</td>\n",
       "      <td>0.190090</td>\n",
       "      <td>0.296503</td>\n",
       "      <td>-0.248052</td>\n",
       "      <td>-0.064512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.260272</td>\n",
       "      <td>-0.949385</td>\n",
       "      <td>1.728538</td>\n",
       "      <td>-0.457986</td>\n",
       "      <td>0.074062</td>\n",
       "      <td>1.419481</td>\n",
       "      <td>0.743511</td>\n",
       "      <td>-0.095576</td>\n",
       "      <td>-0.261297</td>\n",
       "      <td>0.690708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361652</td>\n",
       "      <td>-0.005020</td>\n",
       "      <td>0.702906</td>\n",
       "      <td>0.945045</td>\n",
       "      <td>-1.154666</td>\n",
       "      <td>-0.605564</td>\n",
       "      <td>-0.312895</td>\n",
       "      <td>-0.300258</td>\n",
       "      <td>-0.244718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.152152</td>\n",
       "      <td>-0.508959</td>\n",
       "      <td>1.746840</td>\n",
       "      <td>-1.090178</td>\n",
       "      <td>0.249486</td>\n",
       "      <td>1.143312</td>\n",
       "      <td>0.518269</td>\n",
       "      <td>-0.065130</td>\n",
       "      <td>-0.205698</td>\n",
       "      <td>0.575231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.378223</td>\n",
       "      <td>-0.146927</td>\n",
       "      <td>-0.038212</td>\n",
       "      <td>-0.214048</td>\n",
       "      <td>-1.893131</td>\n",
       "      <td>1.003963</td>\n",
       "      <td>-0.515950</td>\n",
       "      <td>-0.165316</td>\n",
       "      <td>0.048424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.206820</td>\n",
       "      <td>-0.165280</td>\n",
       "      <td>1.527053</td>\n",
       "      <td>-0.448293</td>\n",
       "      <td>0.106125</td>\n",
       "      <td>0.530549</td>\n",
       "      <td>0.658849</td>\n",
       "      <td>-0.212660</td>\n",
       "      <td>1.049921</td>\n",
       "      <td>0.968046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247237</td>\n",
       "      <td>-0.106984</td>\n",
       "      <td>0.729727</td>\n",
       "      <td>-0.161666</td>\n",
       "      <td>0.312561</td>\n",
       "      <td>-0.414116</td>\n",
       "      <td>1.071126</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.419117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568625</th>\n",
       "      <td>-0.833437</td>\n",
       "      <td>0.061886</td>\n",
       "      <td>-0.899794</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-1.002401</td>\n",
       "      <td>0.481454</td>\n",
       "      <td>-0.370393</td>\n",
       "      <td>0.189694</td>\n",
       "      <td>-0.938153</td>\n",
       "      <td>-1.161847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751011</td>\n",
       "      <td>0.167503</td>\n",
       "      <td>0.419731</td>\n",
       "      <td>1.288249</td>\n",
       "      <td>-0.900861</td>\n",
       "      <td>0.560661</td>\n",
       "      <td>-0.006018</td>\n",
       "      <td>3.308968</td>\n",
       "      <td>0.081564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568626</th>\n",
       "      <td>-0.670459</td>\n",
       "      <td>-0.202896</td>\n",
       "      <td>-0.068129</td>\n",
       "      <td>-0.267328</td>\n",
       "      <td>-0.133660</td>\n",
       "      <td>0.237148</td>\n",
       "      <td>-0.016935</td>\n",
       "      <td>-0.147733</td>\n",
       "      <td>0.483894</td>\n",
       "      <td>-0.210817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.550260</td>\n",
       "      <td>0.031874</td>\n",
       "      <td>0.388161</td>\n",
       "      <td>-0.154257</td>\n",
       "      <td>-0.846452</td>\n",
       "      <td>-0.153443</td>\n",
       "      <td>1.961398</td>\n",
       "      <td>-1.528642</td>\n",
       "      <td>1.704306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627</th>\n",
       "      <td>-0.311997</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>0.137526</td>\n",
       "      <td>-0.035893</td>\n",
       "      <td>-0.042291</td>\n",
       "      <td>0.121098</td>\n",
       "      <td>-0.070958</td>\n",
       "      <td>-0.019997</td>\n",
       "      <td>-0.122048</td>\n",
       "      <td>-0.144495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076417</td>\n",
       "      <td>0.140788</td>\n",
       "      <td>0.536523</td>\n",
       "      <td>-0.211100</td>\n",
       "      <td>-0.448909</td>\n",
       "      <td>0.540073</td>\n",
       "      <td>-0.755836</td>\n",
       "      <td>-0.487540</td>\n",
       "      <td>-0.268741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568628</th>\n",
       "      <td>0.636871</td>\n",
       "      <td>-0.516970</td>\n",
       "      <td>-0.300889</td>\n",
       "      <td>-0.144480</td>\n",
       "      <td>0.131042</td>\n",
       "      <td>-0.294148</td>\n",
       "      <td>0.580568</td>\n",
       "      <td>-0.207723</td>\n",
       "      <td>0.893527</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288186</td>\n",
       "      <td>-0.060381</td>\n",
       "      <td>-0.195609</td>\n",
       "      <td>-0.175488</td>\n",
       "      <td>-0.554643</td>\n",
       "      <td>-0.099669</td>\n",
       "      <td>-1.434931</td>\n",
       "      <td>-0.159269</td>\n",
       "      <td>-0.076251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568629</th>\n",
       "      <td>-0.795144</td>\n",
       "      <td>0.433236</td>\n",
       "      <td>-0.649140</td>\n",
       "      <td>0.374732</td>\n",
       "      <td>-0.244976</td>\n",
       "      <td>-0.603493</td>\n",
       "      <td>-0.347613</td>\n",
       "      <td>-0.340814</td>\n",
       "      <td>0.253971</td>\n",
       "      <td>-0.513556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621378</td>\n",
       "      <td>0.534853</td>\n",
       "      <td>-0.291514</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.931030</td>\n",
       "      <td>-0.349423</td>\n",
       "      <td>-1.090974</td>\n",
       "      <td>-1.575113</td>\n",
       "      <td>0.722936</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0      -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
       "1       0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
       "2      -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
       "3      -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
       "4      -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "568625 -0.833437  0.061886 -0.899794  0.904227 -1.002401  0.481454 -0.370393   \n",
       "568626 -0.670459 -0.202896 -0.068129 -0.267328 -0.133660  0.237148 -0.016935   \n",
       "568627 -0.311997 -0.004095  0.137526 -0.035893 -0.042291  0.121098 -0.070958   \n",
       "568628  0.636871 -0.516970 -0.300889 -0.144480  0.131042 -0.294148  0.580568   \n",
       "568629 -0.795144  0.433236 -0.649140  0.374732 -0.244976 -0.603493 -0.347613   \n",
       "\n",
       "              V8        V9       V10  ...       V20       V21       V22  \\\n",
       "0      -0.130006  0.727159  0.637735  ...  0.091202 -0.110552  0.217606   \n",
       "1      -0.133118  0.347452  0.529808  ... -0.233984 -0.194936 -0.605761   \n",
       "2      -0.095576 -0.261297  0.690708  ...  0.361652 -0.005020  0.702906   \n",
       "3      -0.065130 -0.205698  0.575231  ... -0.378223 -0.146927 -0.038212   \n",
       "4      -0.212660  1.049921  0.968046  ...  0.247237 -0.106984  0.729727   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "568625  0.189694 -0.938153 -1.161847  ... -0.751011  0.167503  0.419731   \n",
       "568626 -0.147733  0.483894 -0.210817  ... -0.550260  0.031874  0.388161   \n",
       "568627 -0.019997 -0.122048 -0.144495  ... -0.076417  0.140788  0.536523   \n",
       "568628 -0.207723  0.893527 -0.080078  ...  0.288186 -0.060381 -0.195609   \n",
       "568629 -0.340814  0.253971 -0.513556  ... -0.621378  0.534853 -0.291514   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Class  \n",
       "0      -0.134794  0.165959  0.126280 -0.434824 -0.081230 -0.151045      0  \n",
       "1       0.079469 -0.577395  0.190090  0.296503 -0.248052 -0.064512      0  \n",
       "2       0.945045 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718      0  \n",
       "3      -0.214048 -1.893131  1.003963 -0.515950 -0.165316  0.048424      0  \n",
       "4      -0.161666  0.312561 -0.414116  1.071126  0.023712  0.419117      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "568625  1.288249 -0.900861  0.560661 -0.006018  3.308968  0.081564      1  \n",
       "568626 -0.154257 -0.846452 -0.153443  1.961398 -1.528642  1.704306      1  \n",
       "568627 -0.211100 -0.448909  0.540073 -0.755836 -0.487540 -0.268741      1  \n",
       "568628 -0.175488 -0.554643 -0.099669 -1.434931 -0.159269 -0.076251      1  \n",
       "568629  0.157303  0.931030 -0.349423 -1.090974 -1.575113  0.722936      1  \n",
       "\n",
       "[568630 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2169dce2-e990-4200-9f41-31d23d067d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.iloc[:,:-1]\n",
    "y_label = data.iloc[:,-1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_label,\n",
    "                                                   test_size = 0.2, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3530f0e6-b48f-46a5-9c3a-1d0fc61835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e4d7d1-070b-4a52-a372-2ad6e074ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6276d3d4-30d2-43b5-a5cb-f4432a7adf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 1. 1.] ,  200\n"
     ]
    }
   ],
   "source": [
    "batch_class, batch_labels = next(iter(train_loader))\n",
    "print(batch_labels.numpy(), \", \", len(batch_labels.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d5d710-5aaf-4180-be36-27998d67b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(28, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a8404f-fa28-406d-b31b-798b5d78e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d2602a-ca8d-465f-a94b-b9fede8b64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de11008e-9f4e-4def-8c32-04d2d7f2f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], lter [300/2274], Loss: 0.5438\n",
      "Epoch [1/5], lter [600/2274], Loss: 0.4208\n",
      "Epoch [1/5], lter [900/2274], Loss: 0.3505\n",
      "Epoch [1/5], lter [1200/2274], Loss: 0.2867\n",
      "Epoch [1/5], lter [1500/2274], Loss: 0.2922\n",
      "Epoch [1/5], lter [1800/2274], Loss: 0.2692\n",
      "Epoch [1/5], lter [2100/2274], Loss: 0.2655\n",
      "Epoch [2/5], lter [300/2274], Loss: 0.2430\n",
      "Epoch [2/5], lter [600/2274], Loss: 0.2435\n",
      "Epoch [2/5], lter [900/2274], Loss: 0.1948\n",
      "Epoch [2/5], lter [1200/2274], Loss: 0.2317\n",
      "Epoch [2/5], lter [1500/2274], Loss: 0.1773\n",
      "Epoch [2/5], lter [1800/2274], Loss: 0.1642\n",
      "Epoch [2/5], lter [2100/2274], Loss: 0.1670\n",
      "Epoch [3/5], lter [300/2274], Loss: 0.1626\n",
      "Epoch [3/5], lter [600/2274], Loss: 0.1692\n",
      "Epoch [3/5], lter [900/2274], Loss: 0.1528\n",
      "Epoch [3/5], lter [1200/2274], Loss: 0.1817\n",
      "Epoch [3/5], lter [1500/2274], Loss: 0.1629\n",
      "Epoch [3/5], lter [1800/2274], Loss: 0.1171\n",
      "Epoch [3/5], lter [2100/2274], Loss: 0.1356\n",
      "Epoch [4/5], lter [300/2274], Loss: 0.1748\n",
      "Epoch [4/5], lter [600/2274], Loss: 0.1640\n",
      "Epoch [4/5], lter [900/2274], Loss: 0.1287\n",
      "Epoch [4/5], lter [1200/2274], Loss: 0.1290\n",
      "Epoch [4/5], lter [1500/2274], Loss: 0.1707\n",
      "Epoch [4/5], lter [1800/2274], Loss: 0.1245\n",
      "Epoch [4/5], lter [2100/2274], Loss: 0.1332\n",
      "Epoch [5/5], lter [300/2274], Loss: 0.1328\n",
      "Epoch [5/5], lter [600/2274], Loss: 0.1827\n",
      "Epoch [5/5], lter [900/2274], Loss: 0.1307\n",
      "Epoch [5/5], lter [1200/2274], Loss: 0.1299\n",
      "Epoch [5/5], lter [1500/2274], Loss: 0.1575\n",
      "Epoch [5/5], lter [1800/2274], Loss: 0.1198\n",
      "Epoch [5/5], lter [2100/2274], Loss: 0.1803\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_batch = len(x_train) // batch_size\n",
    "    \n",
    "    for i, (batch_class, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        X = batch_class.view(-1, 28)\n",
    "        Y = batch_labels\n",
    "        \n",
    "        pred = model(X)\n",
    "        Y = Y.long()\n",
    "        cost = loss(pred, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255d2f10-552b-44ff-8032-f5b670e8220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test classes: 95.014333 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for classes, labels in test_dataset:\n",
    "    \n",
    "    classes  = classes.view(-1, 28)\n",
    "    outputs = model(classes)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += 1\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of test classes: %f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
